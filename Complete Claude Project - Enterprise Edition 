# Complete Claude API Project - Enterprise Edition
## Based on Anthropic's Official Quickstarts & Best Practices

This is a **production-ready, enterprise-grade** folder structure combining Anthropic's official quickstart patterns with advanced features like budget compliance, tool orchestration, and multi-model support.

---

## Complete Folder Structure

```
claude-enterprise-project/
├── README.md
├── LICENSE
├── .env.example
├── .gitignore
├── docker-compose.yml
├── Dockerfile
├── pyproject.toml
├── requirements.txt
├── setup.py
│
├── config/
│   ├── __init__.py
│   ├── settings.py
│   ├── models.json
│   ├── budget_limits.json
│   └── tool_permissions.json
│
├── src/
│   ├── __init__.py
│   ├── main.py
│   │
│   ├── api/
│   │   ├── __init__.py
│   │   ├── client.py
│   │   ├── streaming.py
│   │   └── batch.py
│   │
│   ├── orchestrator/
│   │   ├── __init__.py
│   │   ├── agent.py
│   │   ├── planner.py
│   │   └── executor.py
│   │
│   ├── tools/
│   │   ├── __init__.py
│   │   ├── registry.py
│   │   ├── base_tool.py
│   │   ├── web_search.py
│   │   ├── code_executor.py
│   │   ├── data_processor.py
│   │   ├── file_handler.py
│   │   └── database_query.py
│   │
│   ├── budget/
│   │   ├── __init__.py
│   │   ├── tracker.py
│   │   ├── compliance.py
│   │   ├── analytics.py
│   │   └── reporter.py
│   │
│   ├── cache/
│   │   ├── __init__.py
│   │   ├── prompt_cache.py
│   │   └── response_cache.py
│   │
│   ├── knowledge/
│   │   ├── __init__.py
│   │   ├── rag.py
│   │   ├── embeddings.py
│   │   └── vector_store.py
│   │
│   ├── security/
│   │   ├── __init__.py
│   │   ├── auth.py
│   │   ├── encryption.py
│   │   └── audit_log.py
│   │
│   └── utils/
│       ├── __init__.py
│       ├── logger.py
│       ├── validators.py
│       ├── parsers.py
│       └── helpers.py
│
├── skills/
│   ├── README.md
│   ├── custom-skill/
│   │   └── SKILL.md
│   ├── data-analysis/
│   │   └── SKILL.md
│   ├── code-review/
│   │   └── SKILL.md
│   └── document-generation/
│       └── SKILL.md
│
├── apps/
│   ├── customer-support/
│   │   ├── app.py
│   │   ├── requirements.txt
│   │   └── README.md
│   │
│   ├── financial-analyst/
│   │   ├── app.py
│   │   ├── requirements.txt
│   │   └── README.md
│   │
│   └── computer-use-demo/
│       ├── docker-compose.yml
│       ├── Dockerfile
│       └── README.md
│
├── tests/
│   ├── __init__.py
│   ├── conftest.py
│   ├── unit/
│   │   ├── test_orchestrator.py
│   │   ├── test_tools.py
│   │   ├── test_budget.py
│   │   └── test_cache.py
│   ├── integration/
│   │   ├── test_api.py
│   │   ├── test_workflows.py
│   │   └── test_rag.py
│   └── e2e/
│       └── test_complete_workflow.py
│
├── scripts/
│   ├── setup.sh
│   ├── deploy.sh
│   ├── migrate_data.py
│   └── generate_report.py
│
├── data/
│   ├── knowledge_base/
│   ├── embeddings/
│   └── cache/
│
├── logs/
│   ├── .gitkeep
│   ├── api/
│   ├── budget/
│   └── security/
│
├── docs/
│   ├── architecture.md
│   ├── api_reference.md
│   ├── deployment.md
│   ├── security.md
│   └── tutorials/
│       ├── getting_started.md
│       ├── tool_creation.md
│       └── rag_setup.md
│
└── monitoring/
    ├── prometheus.yml
    ├── grafana/
    │   └── dashboards/
    └── alerts/
        └── rules.yml
```

---

## Core Configuration Files

### pyproject.toml
```toml
[tool.poetry]
name = "claude-enterprise"
version = "2.0.0"
description = "Enterprise-grade Claude API orchestration platform"
authors = ["Your Team <team@company.com>"]

[tool.poetry.dependencies]
python = "^3.10"
anthropic = "^0.40.0"
pydantic = "^2.0.0"
pydantic-settings = "^2.0.0"
python-dotenv = "^1.0.0"
aiohttp = "^3.9.0"
asyncio = "^3.4.3"
redis = "^5.0.0"
sqlalchemy = "^2.0.0"
alembic = "^1.13.0"
fastapi = "^0.104.0"
uvicorn = "^0.24.0"
langchain = "^0.1.0"
chromadb = "^0.4.0"
pytest = "^7.4.0"
pytest-asyncio = "^0.21.0"
pytest-cov = "^4.1.0"
black = "^23.11.0"
ruff = "^0.1.6"

[tool.poetry.dev-dependencies]
ipython = "^8.17.0"
jupyter = "^1.0.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

### .env.example
```bash
# API Configuration
ANTHROPIC_API_KEY=your_api_key_here
API_PROVIDER=anthropic  # Options: anthropic, bedrock, vertex

# AWS Bedrock (if using)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_SESSION_TOKEN=
AWS_REGION=us-east-1

# Google Vertex (if using)
VERTEX_PROJECT_ID=
VERTEX_REGION=us-central1

# Model Configuration
PRIMARY_MODEL=claude-sonnet-4-20250514
FALLBACK_MODEL=claude-haiku-4-20251001
MAX_TOKENS=4096
TEMPERATURE=0.7

# Budget Limits
BUDGET_LIMIT_DAILY=100.00
BUDGET_LIMIT_MONTHLY=3000.00
BUDGET_ALERT_THRESHOLD=0.80

# Cache Configuration
ENABLE_PROMPT_CACHE=true
REDIS_URL=redis://localhost:6379
CACHE_TTL=3600

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/claude_db

# Security
ALLOWED_CALLERS=service_a,service_b
CODE_EXECUTION_ENABLED=true
SANDBOX_MODE=true
AUDIT_LOG_ENABLED=true

# Tool Permissions
ENABLE_WEB_SEARCH=true
ENABLE_CODE_EXECUTION=true
ENABLE_FILE_ACCESS=true
ENABLE_DATABASE_QUERY=false

# RAG Configuration
VECTOR_STORE_TYPE=chroma
EMBEDDING_MODEL=voyage-2
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Monitoring
LOG_LEVEL=INFO
METRICS_ENABLED=true
PROMETHEUS_PORT=9090

# Application Settings
APP_NAME=claude-enterprise
ENVIRONMENT=development
DEBUG=false
```

### docker-compose.yml
```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/claude_db
    depends_on:
      - redis
      - db
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./logs:/app/logs

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=claude_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards

volumes:
  redis_data:
  postgres_data:
  prometheus_data:
  grafana_data:
```

### Dockerfile
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Poetry
RUN curl -sSL https://install.python-poetry.org | python3 -

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Install dependencies
RUN poetry config virtualenvs.create false \
    && poetry install --no-interaction --no-ansi

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p logs data/knowledge_base data/embeddings data/cache

EXPOSE 8000

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## Enhanced Core Implementation

### config/models.json
```json
{
  "models": {
    "claude-sonnet-4-20250514": {
      "display_name": "Claude Sonnet 4",
      "context_window": 200000,
      "max_output": 8192,
      "input_cost_per_million": 3.00,
      "output_cost_per_million": 15.00,
      "cache_write_per_million": 3.75,
      "cache_read_per_million": 0.30,
      "supports_vision": true,
      "supports_tools": true,
      "supports_streaming": true,
      "recommended_for": ["general", "coding", "analysis"]
    },
    "claude-opus-4-20251101": {
      "display_name": "Claude Opus 4",
      "context_window": 200000,
      "max_output": 8192,
      "input_cost_per_million": 15.00,
      "output_cost_per_million": 75.00,
      "cache_write_per_million": 18.75,
      "cache_read_per_million": 1.50,
      "supports_vision": true,
      "supports_tools": true,
      "supports_streaming": true,
      "recommended_for": ["complex_reasoning", "research", "critical_tasks"]
    },
    "claude-haiku-4-20251001": {
      "display_name": "Claude Haiku 4",
      "context_window": 200000,
      "max_output": 8192,
      "input_cost_per_million": 0.80,
      "output_cost_per_million": 4.00,
      "cache_write_per_million": 1.00,
      "cache_read_per_million": 0.08,
      "supports_vision": true,
      "supports_tools": true,
      "supports_streaming": true,
      "recommended_for": ["fast_responses", "simple_tasks", "high_volume"]
    }
  },
  "fallback_strategy": {
    "enabled": true,
    "order": [
      "claude-sonnet-4-20250514",
      "claude-haiku-4-20251001"
    ]
  }
}
```

### src/api/client.py
```python
"""Enhanced Claude API Client with multi-provider support"""

import os
from typing import Optional, List, Dict, Any, AsyncIterator
from anthropic import Anthropic, AsyncAnthropic
import boto3
from enum import Enum

class APIProvider(Enum):
    ANTHROPIC = "anthropic"
    BEDROCK = "bedrock"
    VERTEX = "vertex"

class ClaudeClient:
    """Unified client for Claude API across providers"""
    
    def __init__(
        self,
        provider: APIProvider = APIProvider.ANTHROPIC,
        api_key: Optional[str] = None,
        region: Optional[str] = None
    ):
        self.provider = provider
        
        if provider == APIProvider.ANTHROPIC:
            self.client = AsyncAnthropic(api_key=api_key or os.getenv("ANTHROPIC_API_KEY"))
        elif provider == APIProvider.BEDROCK:
            self.bedrock = boto3.client(
                service_name='bedrock-runtime',
                region_name=region or os.getenv("AWS_REGION")
            )
        elif provider == APIProvider.VERTEX:
            # Vertex AI setup
            from anthropic import AnthropicVertex
            self.client = AnthropicVertex(
                region=region or os.getenv("VERTEX_REGION"),
                project_id=os.getenv("VERTEX_PROJECT_ID")
            )
    
    async def create_message(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        max_tokens: int = 4096,
        temperature: float = 0.7,
        system: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        stream: bool = False
    ) -> Any:
        """Create message with unified interface"""
        
        kwargs = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        if system:
            kwargs["system"] = system
        if tools:
            kwargs["tools"] = tools
            
        if stream:
            return await self._stream_message(**kwargs)
        else:
            return await self.client.messages.create(**kwargs)
    
    async def _stream_message(self, **kwargs) -> AsyncIterator[Any]:
        """Stream message response"""
        async with self.client.messages.stream(**kwargs) as stream:
            async for chunk in stream:
                yield chunk
    
    async def create_batch(
        self,
        requests: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Create batch processing request"""
        return await self.client.messages.batches.create(requests=requests)
    
    async def get_batch_status(self, batch_id: str) -> Dict[str, Any]:
        """Get batch processing status"""
        return await self.client.messages.batches.retrieve(batch_id)
```

### src/orchestrator/agent.py
```python
"""Advanced agent orchestration with planning and execution"""

import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime

from src.api.client import ClaudeClient
from src.tools.registry import ToolRegistry
from src.budget.tracker import BudgetTracker
from src.cache.prompt_cache import PromptCache
from src.utils.logger import setup_logger

logger = setup_logger(__name__)

class AgentOrchestrator:
    """Orchestrates complex multi-step agent workflows"""
    
    def __init__(
        self,
        client: ClaudeClient,
        model: str,
        enable_caching: bool = True
    ):
        self.client = client
        self.model = model
        self.tool_registry = ToolRegistry()
        self.budget_tracker = BudgetTracker()
        self.cache = PromptCache() if enable_caching else None
        self.conversation_history: List[Dict] = []
        
    async def execute_task(
        self,
        task: str,
        context: Optional[Dict[str, Any]] = None,
        max_iterations: int = 10,
        enable_tools: bool = True
    ) -> Dict[str, Any]:
        """Execute a complex task with planning and tool use"""
        
        # Check budget
        if not self.budget_tracker.can_execute():
            raise Exception("Budget limit exceeded")
        
        # Check cache
        if self.cache:
            cached_result = await self.cache.get(task, context)
            if cached_result:
                logger.info("Returning cached result")
                return cached_result
        
        # Initialize conversation
        messages = self._prepare_messages(task, context)
        
        # Get available tools
        tools = self.tool_registry.get_all_tools() if enable_tools else None
        
        iteration = 0
        while iteration < max_iterations:
            iteration += 1
            logger.info(f"Iteration {iteration}/{max_iterations}")
            
            try:
                response = await self.client.create_message(
                    model=self.model,
                    messages=messages,
                    tools=tools,
                    max_tokens=4096
                )
                
                # Track usage
                self.budget_tracker.track_usage(response.usage)
                
                # Process response
                if response.stop_reason == "end_turn":
                    result = self._extract_result(response)
                    
                    # Cache successful result
                    if self.cache:
                        await self.cache.set(task, context, result)
                    
                    return result
                
                elif response.stop_reason == "tool_use":
                    # Execute tools
                    tool_results = await self._execute_tools(response.content)
                    
                    # Add tool results to conversation
                    messages.append({"role": "assistant", "content": response.content})
                    messages.append({"role": "user", "content": tool_results})
                    
                else:
                    logger.warning(f"Unexpected stop reason: {response.stop_reason}")
                    break
                    
            except Exception as e:
                logger.error(f"Error in iteration {iteration}: {e}")
                raise
        
        raise Exception(f"Max iterations ({max_iterations}) reached without completion")
    
    async def _execute_tools(self, content: List[Dict]) -> List[Dict]:
        """Execute tools in parallel"""
        tool_uses = [block for block in content if block.get("type") == "tool_use"]
        
        tasks = [
            self.tool_registry.execute_tool(
                tool_use["name"],
                tool_use["input"]
            )
            for tool_use in tool_uses
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return [
            {
                "type": "tool_result",
                "tool_use_id": tool_use["id"],
                "content": str(result) if not isinstance(result, Exception) else f"Error: {result}"
            }
            for tool_use, result in zip(tool_uses, results)
        ]
    
    def _prepare_messages(
        self,
        task: str,
        context: Optional[Dict[str, Any]]
    ) -> List[Dict]:
        """Prepare initial messages with context"""
        messages = []
        
        if context:
            context_str = "\n".join([f"{k}: {v}" for k, v in context.items()])
            messages.append({
                "role": "user",
                "content": f"Context:\n{context_str}\n\nTask: {task}"
            })
        else:
            messages.append({
                "role": "user",
                "content": task
            })
        
        return messages
    
    def _extract_result(self, response: Any) -> Dict[str, Any]:
        """Extract final result from response"""
        return {
            "content": response.content[0].text if response.content else "",
            "usage": {
                "input_tokens": response.usage.input_tokens,
                "output_tokens": response.usage.output_tokens,
                "cache_read_tokens": getattr(response.usage, "cache_read_input_tokens", 0),
                "cache_creation_tokens": getattr(response.usage, "cache_creation_input_tokens", 0)
            },
            "cost": self.budget_tracker.get_session_cost(),
            "timestamp": datetime.utcnow().isoformat()
        }
```

### src/cache/prompt_cache.py
```python
"""Prompt caching for cost optimization"""

import hashlib
import json
from typing import Optional, Dict, Any
import redis.asyncio as redis
from src.utils.logger import setup_logger

logger = setup_logger(__name__)

class PromptCache:
    """Cache for prompt responses using Redis"""
    
    def __init__(self, redis_url: str = None, ttl: int = 3600):
        self.redis_url = redis_url or "redis://localhost:6379"
        self.ttl = ttl
        self.client: Optional[redis.Redis] = None
    
    async def connect(self):
        """Connect to Redis"""
        if not self.client:
            self.client = await redis.from_url(self.redis_url)
    
    async def get(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Optional[Dict[str, Any]]:
        """Get cached response"""
        await self.connect()
        
        cache_key = self._generate_key(prompt, context)
        
        try:
            cached = await self.client.get(cache_key)
            if cached:
                logger.info(f"Cache hit for key: {cache_key[:16]}...")
                return json.loads(cached)
        except Exception as e:
            logger.error(f"Cache get error: {e}")
        
        return None
    
    async def set(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]],
        result: Dict[str, Any]
    ):
        """Set cached response"""
        await self.connect()
        
        cache_key = self._generate_key(prompt, context)
        
        try:
            await self.client.setex(
                cache_key,
                self.ttl,
                json.dumps(result)
            )
            logger.info(f"Cached result for key: {cache_key[:16]}...")
        except Exception as e:
            logger.error(f"Cache set error: {e}")
    
    def _generate_key(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]]
    ) -> str:
        """Generate cache key from prompt and context"""
        content = prompt
        if context:
            content += json.dumps(context, sort_keys=True)
        
        return f"claude_cache:{hashlib.sha256(content.encode()).hexdigest()}"
```

### src/knowledge/rag.py
```python
"""RAG (Retrieval Augmented Generation) implementation"""

from typing import List, Dict, Any, Optional
import chromadb
from chromadb.config import Settings

from src.utils.logger import setup_logger

logger = setup_logger(__name__)

class RAGSystem:
    """RAG system for knowledge retrieval"""
    
    def __init__(
        self,
        collection_name: str = "claude_knowledge",
        persist_directory: str = "./data/embeddings"
    ):
        self.client = chromadb.C