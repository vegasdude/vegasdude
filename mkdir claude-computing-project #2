# Local LLM Setup Guide

## Running AI Models on Your Own Hardware

This guide covers setting up local Large Language Models (LLMs) that run entirely on your computer, using your own GPU/CPU.

---

## Option 1: Ollama (Easiest)

**Best for:** Beginners, quick setup, Mac/Linux/Windows

### Installation
```bash
# Visit https://ollama.com/download
# Or use command line:

# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows: Download installer from website
```

### Usage
```bash
# Download and run a model
ollama run llama3.2

# List available models
ollama list

# Popular models:
ollama run llama3.2:3b     # Fast, 2GB RAM
ollama run mistral         # Balanced, 4GB RAM
ollama run llama3.2:70b    # Powerful, 40GB RAM (needs good GPU)
```

### Python Integration
```python
# pip install ollama
import ollama

response = ollama.chat(model='llama3.2', messages=[
  {'role': 'user', 'content': 'Why is the sky blue?'}
])
print(response['message']['content'])
```

---

## Option 2: LM Studio (GUI Interface)

**Best for:** Non-technical users, visual interface

1. Download from: https://lmstudio.ai/
2. Open app and browse model library
3. Download models (Llama, Mistral, etc.)
4. Chat interface built-in
5. Can run local API server

**System Requirements:**
- 8GB+ RAM for small models
- 16GB+ RAM for medium models
- NVIDIA/AMD GPU recommended (not required)

---

## Option 3: Text Generation WebUI

**Best for:** Advanced users, maximum control

```bash
# Clone repository
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui

# Run installer
# Windows: start_windows.bat
# Linux: ./start_linux.sh
# macOS: ./start_macos.sh
```

Access at: http://localhost:7860

**Features:**
- Multiple model support
- Extensions and plugins
- API server mode
- Fine-tuning capabilities

---

## GPU Requirements Comparison

| Model Size | RAM Needed | GPU VRAM | Speed | Quality |
|------------|-----------|----------|-------|---------|
| 3B params  | 4GB       | 2GB      | Fast  | Basic   |
| 7B params  | 8GB       | 4GB      | Good  | Good    |
| 13B params | 16GB      | 8GB      | Medium| Great   |
| 34B params | 32GB      | 20GB     | Slow  | Excellent|
| 70B params | 64GB      | 40GB     | Very Slow | Best |

**PS3 Comparison:** PS3 had 256MB system RAM + 256MB GPU RAM = 512MB total. Modern 7B models need ~16x more RAM!

---

## Checking Your GPU

### NVIDIA GPUs
```bash
nvidia-smi
```

### AMD GPUs
```bash
rocm-smi
```

### Check in Python
```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU name: {torch.cuda.get_device_name(0)}")
print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
```

---

## Popular Models to Try

1. **Llama 3.2** (Meta) - General purpose, very capable
2. **Mistral 7B** - Fast and efficient
3. **Phi-3** (Microsoft) - Small but powerful
4. **CodeLlama** - Specialized for coding
5. **Mixtral 8x7B** - Advanced mixture of experts

---

## Performance Tips

1. **Use quantized models** - 4-bit or 8-bit versions use less RAM
2. **GPU acceleration** - 10-100x faster than CPU
3. **Batch processing** - Process multiple requests together
4. **Model caching** - Keep models loaded in memory
5. **Optimize context length** - Shorter contexts = faster

---

## Troubleshooting

**Out of Memory?**
- Try smaller model (3B instead of 7B)
- Use quantized version (Q4 or Q8)
- Close other applications
- Reduce context window size

**Slow Performance?**
- Ensure GPU is being used (not CPU)
- Update GPU drivers
- Try smaller model
- Check background processes

**Can't find GPU?**
- Install CUDA toolkit (NVIDIA)
- Install ROCm (AMD)
- Verify drivers installed correctly